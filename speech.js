import * as dotenv from 'dotenv'
import { 
  SpeechConfig, 
  AudioConfig, 
  SpeechRecognizer, 
  SpeechSynthesizer, 
  ResultReason, 
  CancellationDetails } from "microsoft-cognitiveservices-speech-sdk";
import FfmpegCommand  from 'fluent-ffmpeg';
import fs from 'fs';

// const { FfmpegCommand } = Ffmpeg;
const ffmpeg = new FfmpegCommand();
dotenv.config()

const { speakbot_token, apiKey, group_name, SPEECH_KEY, SPEECH_REGION } = process.env
const prefix = group_name ? '/' + group_name : '/gpt'
const bot = new TelegramBot(speakbot_token, { polling: true});
console.log(new Date().toLocaleString(), '--Bot has been started...');

const configuration = new Configuration({
  apiKey,
});
const openai = new OpenAIApi(configuration);

// This example requires environment variables named "SPEECH_KEY" and "SPEECH_REGION"
const speechConfig = SpeechConfig.fromSubscription(SPEECH_KEY, SPEECH_REGION);
speechConfig.speechRecognitionLanguage = "en-US";
speechConfig.speechSynthesisLanguage = 'en-US';
speechConfig.speechSynthesisVoiceName = "en-US-JennyNeural"; 

function recognizeVoice(msg, fileName) {
  let audioConfig = AudioConfig.fromWavFileInput(fs.readFileSync(fileName));
  let speechRecognizer = new SpeechRecognizer(speechConfig, audioConfig);

  speechRecognizer.recognizeOnceAsync(result => {
      switch (result.reason) {
          case ResultReason.RecognizedSpeech:
              console.log(`RECOGNIZED: Text=${result.text}`);
              msg.text = result.text;
              msgHandler(msg);
              break;
          case ResultReason.NoMatch:
              console.log("NOMATCH: Speech could not be recognized.");
              break;
          case ResultReason.Canceled:
              const cancellation = CancellationDetails.fromResult(result);
              console.log(`CANCELED: Reason=${cancellation.reason}`);

              if (cancellation.reason == CancellationReason.Error) {
                  console.log(`CANCELED: ErrorCode=${cancellation.ErrorCode}`);
                  console.log(`CANCELED: ErrorDetails=${cancellation.errorDetails}`);
                  console.log("CANCELED: Did you set the speech resource key and region values?");
              }
              break;
      }
      speechRecognizer.close();
      speechRecognizer = null;
  });
}

function synthesizeVoice(text, msg) {
  const chatId = msg.chat.id;
  const msgId = msg.message_id;
  const fileName = `./voiceFiles/${chatId}-${msgId}-res.wav`;
  const outputFileName = `./voiceFiles/${chatId}-${msgId}-res.ogg`;
  const audioConfig = AudioConfig.fromAudioFileOutput(fileName);
  let synthesizer = new SpeechSynthesizer(speechConfig, audioConfig);

  synthesizer.speakTextAsync(text,
    function (result) {
      if (result.reason === ResultReason.SynthesizingAudioCompleted) {
        console.log("synthesis finished.", fileName, ", duration=", result.audioDuration);
        ffmpeg.input(fileName)
            .output(outputFileName)
            .on('end', function() {
              console.log('wavæ–‡ä»¶è½¬æ¢ä¸ºoggæ ¼å¼æˆåŠŸï¼');
              bot.sendVoice(chatId, outputFileName);
            })
            .on('error', function(err) {
              console.error('oggæ–‡ä»¶è½¬æ¢ä¸ºwavæ ¼å¼å¤±è´¥ï¼š' + err.message);
            })
            .run(); 
      } else {
        console.error("Speech synthesis canceled, " + result.errorDetails +
            "\nDid you set the speech resource key and region values?");
      }
      synthesizer.close();
      synthesizer = null;
    },
    function (err) {
      console.trace("err - " + err);
      synthesizer.close();
      synthesizer = null;
    });
}


bot.on('text', async (msg) => {
  console.log(new Date().toLocaleString(), '--Received message from id:', msg.chat.id, ':', msg.text);
  await msgHandler(msg);
});

bot.on('voice', msg => {
  const fileId = msg.voice.file_id;
  const chatId = msg.chat.id;
  const msgId = msg.message_id;
  bot.getFileLink(fileId).then(fileLink => {
    // ä¸‹è½½è¯­éŸ³æ–‡ä»¶
    bot.downloadFile(fileId, './').then(voicePath => {
      const fileName = `./voiceFiles/${chatId}-${msgId}.ogg`;
      const outputFileName = `./voiceFiles/${chatId}-${msgId}.wav`;
      fs.renameSync(voicePath, fileName);
      ffmpeg.input(fileName)
            .output(outputFileName)
            .on('end', function() {
              console.log('oggæ–‡ä»¶è½¬æ¢ä¸ºwavæ ¼å¼æˆåŠŸï¼');
              recognizeVoice(msg, outputFileName);
            })
            .on('error', function(err) {
              console.error('oggæ–‡ä»¶è½¬æ¢ä¸ºwavæ ¼å¼å¤±è´¥ï¼š' + err.message);
            })
            .run();            
    });
  });
});

async function msgHandler(msg) {
  if (typeof msg.text !== 'string' || ((msg.chat.type === 'group' || msg.chat.type === 'supergroup') && !msg.text.startsWith(prefix) && typeof msg.voice === undefined)) {  
    return;
  }
  switch (true) {
    case msg.text.startsWith('/start'):
      await bot.sendMessage(msg.chat.id, 'ğŸ‘‹æ‚¨å¥½ï¼æˆ‘æ˜¯ChatGPTï¼Œå¾ˆé«˜å…´èƒ½ä¸æ‚¨äº¤è°ˆï¼Ÿ');
      break;
    case msg.text.length >= 2:
      await chatGpt(msg, typeof msg.voice !== undefined);
      break;
    default:
      await bot.sendMessage(msg.chat.id, 'ğŸ˜­æˆ‘ä¸å¤ªæ˜ç™½æ‚¨çš„æ„æ€ã€‚');
      break;
  }
}

async function chatGpt(msg, bVoice) {
  try {
    const tempId = (await bot.sendMessage(msg.chat.id, 'ğŸ¤”æ­£åœ¨æ€è€ƒå¹¶ç»„ç»‡è¯­è¨€ï¼Œè¯·ç¨ç­‰...', {
      reply_to_message_id: msg.message_id
    })).message_id;
    //const response = await api.sendMessage(msg.text.replace(prefix, ''))
    await getResponseFromOpenAI(msg, tempId, bVoice);
  } catch (err) {
    console.log('Error:', err)
    await bot.sendMessage(msg.chat.id, 'ğŸ˜­å‡ºé”™äº†ï¼Œè¯·ç¨åå†è¯•ï¼›å¦‚æœæ‚¨æ˜¯ç®¡ç†å‘˜ï¼Œè¯·æ£€æŸ¥æ—¥å¿—ã€‚');
    throw err
  }
}

async function getResponseFromOpenAI(msg, tempId, bVoice) {
  let intervalId;
  try {
    bot.sendChatAction(msg.chat.id, 'typing');
    intervalId = setInterval(() => {
        bot.sendChatAction(msg.chat.id, 'typing');
    }, 5000);
    const res = await openai.createCompletion({
        model: "text-davinci-003",
        prompt: msg.text.startsWith(prefix) ? msg.text.replace(prefix, '') : msg.text,
        max_tokens: bVoice ? 200 : 1000,
        top_p: 1,
        stop: "###",
    }, { responseType: 'json' });
    let resText = res.data.choices[0].text;
    console.log(resText);
    clearInterval(intervalId);
    if (resText.indexOf("\n\n") > 0) {
        resText = resText.substr(resText.indexOf("\n\n") + "\n\n".length);
    }
    if (!bVoice)
      await bot.editMessageText(resText, { parse_mode: 'Markdown', chat_id: msg.chat.id, message_id: tempId });
    else {
      synthesizeVoice(resText, msg);
    }
    return;
  } catch (error) {
      clearInterval(intervalId);
      if (error.response?.status) {
          console.error(error.response.status, error.message);    
          await bot.sendMessage(msg.chat.id, 'ğŸ˜­è¢«é™é€Ÿäº†ï¼Œè¯·ç¨åå†è¯•ï¼Œé”™è¯¯ä»£ç : ' + error.response.status);      
      } else {
          console.error('An error occurred during OpenAI request', error);
          await bot.sendMessage(msg.chat.id, 'ğŸ˜­å‡ºé”™äº†ï¼Œè¯·ç¨åå†è¯•');
      }
  }
}